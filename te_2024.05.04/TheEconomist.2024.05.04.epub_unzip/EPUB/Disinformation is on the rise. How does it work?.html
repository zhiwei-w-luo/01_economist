<?xml version='1.0' encoding='utf-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" epub:prefix="z3998: http://www.daisy.org/z3998/2012/vocab/structure/#" lang="en" xml:lang="en">
  <head>
    <title>Disinformation is on the rise. How does it work?</title>
    <link href="static_styles/default_styles.css" rel="stylesheet" type="text/css"/>
  </head>
  <body><h4 class="te_fly_title">Bad news</h4><br/><h1 class="te_article_title">Disinformation is on the rise. How does it work?</h1><h3 class="te_article_rubric">Understanding it will lead to better ways to fight it</h3><h3 class="te_article_datePublished">May 01th 2024</h3><br/><div class="head_image_div"><img src="static_images/01698eeac0bc405e6d5f0bad0e9a7f8e.jpg" alt="" class="te_head_image"/></div><p><span>In January 2024</span>, in the run-up to elections in Taiwan, hundreds of video posts appeared on YouTube, Instagram, X and other social platforms entitled “The Secret History of Tsai Ing-wen”. News anchors, speaking English and Chinese, made a series of false claims about Ms Tsai, the outgoing president, and her ruling party. On election day itself, January 13th, an audio clip began to circulate in which Terry Gou, a candidate who had dropped out of the race in November, seemed to endorse the candidate of the China-friendly <small>KMT</small> party (in fact, Mr Gou made no endorsement). </p><p>Both the video clips and audio were probably created using artificial intelligence (<small>AI</small>) and posted by a Chinese state-backed propaganda group known variously as Spamouflage, Dragonbridge and Storm-1376. In a report released on April 5th, the Threat Intelligence team at Microsoft, a tech firm, said this was the first time it had seen a nation-state use <small>AI</small>-generated material to sway a foreign election.</p><figure><img src="static_images/fd82ac11143b232b1e3365c055eb2191.png" alt="None" class="te_img"/></figure><aside><p><b>More from this package</b></p><ul><li><a href="https://www.economist.com/leaders/2024/05/02/how-disinformation-works-and-how-to-counter-it">How disinformation works—and how to counter it</a></li><li><a href="https://www.economist.com/science-and-technology/2024/05/01/producing-fake-information-is-getting-easier">Producing fake information is getting easier</a></li><li><a href="https://www.economist.com/interactive/science-and-technology/2024/05/01/the-truth-behind-olena-zelenskas-cartier-haul">The anatomy of a disinformation campaign</a></li><li><a href="https://www.economist.com/science-and-technology/2024/05/01/fighting-disinformation-gets-harder-just-when-it-matters-most">Fighting disinformation gets harder, just when it matters most</a></li></ul></aside><p>The news anchors in the videos were made using CapCut, an app made by ByteDance, the Chinese parent company of TikTok. At their peak, the videos were being shared 100 times a minute, but were swiftly identified and taken down. Overall, few people probably saw them. But China is likely to be using Taiwan as a testbed for ideas it plans to deploy elsewhere, a Taiwanese official told the <i>Taipei Times</i>. Taiwan’s election is a sign of things to come, as <small>AI</small> supercharges the production of disinformation (that is, information that is intended to deceive). The country’s social media are flooded with one of the world’s highest levels of disinformation coming from foreign governments (see chart). American social media are not far behind on the same measure. </p><p>In a year when half the world is holding elections and new technology is making it easier than ever to make and spread disinformation, the need for governments, companies and individuals around the world to grapple with the problem has never been more urgent. Security experts are raising the alarm too—more than 1,400 of them recently told the World Economic Forum that disinformation and misinformation (incorrect information that is shared unwittingly) were the biggest global risks in the next two years, even more dangerous than war, extreme weather or inflation.</p><h2>The fog of information war</h2><p>Much is still murky, including how much disinformation there is and exactly how (and how much) it shapes opinions and behaviour. Nevertheless, researchers are beginning to understand how disinformation networks operate and are developing ways to identify and monitor them in the wild. Some countries—from Taiwan and Sweden to Brazil—have implemented policies to fight the problem, which could provide useful lessons for others. </p><p>This special section will first explain how disinformation campaigns work. It will consider the role of <small>AI</small>—both negative (creating disinformation) and positive (detecting and mitigating it). And it will assess the emerging tools and policies aiming to <a href="https://www.economist.com/science-and-technology/2024/05/01/fighting-disinformation-gets-harder-just-when-it-matters-most">fight back</a> against the problem.</p><p>Bad information can take many forms and affect many fields. For many years the standard way of doing things, says Amil Khan, a former journalist who now studies disinformation, was to use hundreds or thousands of social-media accounts, controlled by a single piece of software, to pump out the same message or link, or to “like” or reshare particular posts. On a large scale, this “co-ordinated inauthentic behaviour” (<small>CIB</small>) can fool the curation algorithms on a social network such as Facebook or X into thinking there is a groundswell of interest in, or support for, a particular viewpoint. The algorithm then promotes those posts to real users, who may then share them with their own followers.</p><p>One example of <small>CIB</small> analysed by Mr Khan’s firm, Valent Projects, targeted Daewoo, a South Korean company. After Daewoo won a construction contract worth $2.6bn from the Iraqi government, it was attacked by Chinese disinformation networks, which spread false stories about the company in an effort to get its contract cancelled and awarded to a Chinese company instead. Daewoo was said to be a front for a Western plan to exploit Iraq’s resources; made-up comments attributed to American officials were cited as evidence that America was trying to undermine Chinese-Iraqi relations. Such claims were debunked by fact-checking outfits based in Iraq and Qatar, but that did little to hamper their spread.</p><p><small>CIB</small> operations are fairly easy to spot. Meta, the tech firm behind Facebook, now finds and shuts down such networks quite quickly, says Mr Khan (though X is slower off the mark, he adds). In its most recent report on <small>CIB</small> shutdowns, released in February, Meta describes three such operations, in China, Myanmar and Ukraine. The report notes that early <small>CIB</small> networks targeting Ukraine were simple and could be traced to Russian intelligence services.</p><p>Since 2022, however, disinformation campaigns have taken on a new form. Run by “deniable entities” such as marketing companies or troll farms without direct state links, they post on a range of social networks and blogging platforms and create entire fake websites. Since May 2023 the number of <small>AI</small>-generated news outlets peddling misleading information has risen from 49 to 802, according to NewsGuard, an American organisation that monitors disinformation. These sites mostly feature innocuous articles, generated by <small>AI</small>, but with disinformation mixed in.</p><p>An example is <i><small>DC</small> Weekly</i>, an apparently American website that was central to furthering the Russian-led disinformation campaign alleging that Olena Zelenska, Ukraine’s first lady, had spent $1.1m on a shopping spree on New York’s swanky Fifth Avenue (see <a href="https://www.economist.com/interactive/science-and-technology/2024/05/01/the-truth-behind-olena-zelenskas-cartier-haul">interactive</a>). That story, which was tracked by researchers at Clemson University, began with a video on YouTube, passed through several African news websites and an <small>AI</small>-generated site before being planted on social media and boosted by Russian propaganda outfits. It was shared 20,000 times on X.</p><p>Mr Khan calls the accounts and sites that plant the story “seeders”. Rather than using hundreds of fake accounts to promote these sites’ material, distribution instead relies on a few so-called “spreaders”—social-media accounts with large numbers of followers. Spreader accounts typically build a following by posting about football, or featuring scantily clad women. “And then they’ll flip,” says Mr Khan: they start mixing in disinformation from seeders, by linking to or reposting their content. Meta’s threat report from November 2023 noted that it had seen the Chinese outfit Spamouflage/Storm-1376 operating on more than 50 platforms, “and it primarily seeded content on blogging platforms and forums like Medium, Reddit and Quora before sharing links to that content on [our platforms]”.</p><p>In poor countries with few opportunities for young, tech-savvy men, there is a cottage industry of building up spreader accounts and then selling them to malicious actors once they reach 100,000 followers, says Mr Khan. The challenge with identifying spreaders is that their behaviour is genuine, at least to begin with, and they are not the originators of disinformation, but merely distributors of it. Spreader accounts may continue to post about other things, with disinformation mixed in every so often, to avoid detection.</p><p>Valent has seen this more sophisticated approach being used to spread disinformation from Russia in European countries, and to promote hard-right material in Britain. In the latter case, the spreader accounts used gossipy posts about the British royal family to attract followers, before flipping to political propaganda about how “low-traffic neighbourhoods” (areas where through-traffic is discouraged) are a globalist plot. Similarly, Microsoft has detailed Storm-1376’s use of this newer distribution model to spread disinformation about wildfires in Hawaii (supposedly started by an American “weather weapon”), to amplify criticism of the Japanese government in South Korea, and to stoke conspiracy theories about a train derailment in Kentucky in November 2023.</p><p>When many accounts are using exactly the same wording, spotting <small>CIB</small> is relatively simple. It is not unusual for narratives to suddenly “trend” on social media, but a spike in mentions of a particular topic in an array of different languages, or posted by accounts seemingly scattered around the world, might be a hint that foul play is involved. Similarly, disinformation hunters can examine clusters of accounts that push a similar message. Dodgy accounts may all have the same date of creation, the same number of followers or the same ratio of followers to following (because they have bought fake followers in bulk in a bid to look authentic).</p><p>But spotting seeders and spreaders under the newer distribution model is more difficult. They are propagating a particular narrative, but the seeder articles and posts, and the spreader posts that promote them, may all use different wording. Valent is trying to solve this problem using <small>AI</small>: its system, called Ariadne, takes in feeds from social platforms and looks for common underlying narratives and sentiments to spot unusual, co-ordinated action. Unlike previous approaches based on keywords, “the latest models let us do work on sentiment that we couldn’t do before”, says Mr Khan.</p><p>Another way to identify spreader accounts is described in a recent working paper from Brookings, a think-tank based in Washington, <small>DC</small>. Maryam Saeedi, an economist at Carnegie Mellon University, and her colleagues analysed 62m posts from X written in Farsi, relating to the wave of anti-government protests in Iran that began in September 2022. The analysis focused on spreader accounts (the researchers call them “imposters”) that started off by pretending to be on the side of the protesters, but then flipped to posting disinformation discrediting the protests.</p><p>The researchers began by identifying several hundred imposter accounts by hand. They then trained a classifier algorithm to identify more imposters with similar characteristics, including their posting activity, the pattern of their followers, their use of particular hashtags, how recently the account was created, and so on. The researchers were then able to replicate the identification of these imposter accounts, with 94% accuracy, through network-analysis alone—ie, by scrutinising only their relationship to other accounts, rather than the content of their posts.</p><p>This suggests, the researchers say, that it is possible to identify “accounts with a high propensity to engage in disinformation campaigns, even before they do so”. They suggest that social-media platforms could use these kinds of network-analysis methods to calculate a “propaganda score” for each account, made visible to users, to indicate whether it is likely to be a source of disinformation. The imposter-detection algorithm could be further improved, the researchers suggest, using more advanced forms of <small>AI</small> such as natural-language processing and deep learning.</p><p>Technology firms and intelligence agencies are no doubt already doing this kind of analysis, though they are understandably reluctant to share details about their methods. Meta says only that it has used <small>AI</small> in its “integrity systems” for many years to protect users and enforce its rules. But both academics and civil-society groups say that no single method can be used to automatically detect all disinformation—the tactics used are often bespoke to specific campaigns and rely on human analysts to check the results and provide interpretation and nuance.</p><p><small>AI</small> can, however, help in a different way: by spotting deceptive content directly, through analysis of individual posts, articles, sound clips or videos. <small>DARPA</small>, the special-projects research arm of America’s Department of Defence, has been funding research into “detecting, attributing and characterising manipulated and synthesised media” as part of its “semantic forensics” programme, to create a toolbox of defences. In March it published an open-source repository of several of the projects it has funded, with links to downloadable source code, and announced a series of “spot the deepfake” challenges. Its aim is to encourage academic and commercial users to combine, improve and ultimately deploy these tools, all of which rely on <small>AI</small> in some form, says Wil Corvey of <small>DARPA</small>, who manages the programme.</p><p>Although a single analytic tool may not always be reliable, he says, combining several of them can greatly improve accuracy. Consider, for example, the problem of working out whether a video of a politician is genuine or not. Using authentic data of the person in question, it is possible to train an <small>AI</small> model that learns their characteristics, such as patterns of head tilt or facial movements while speaking. This can then be used to analyse a suspected deepfake for authenticity. Better still, explains Dr Corvey, it can be combined with other techniques, such as heartbeat detection from video, which is difficult to fake. (Heartbeats can be spotted by looking for tiny variations in skin colour, particularly on the forehead.) Other tools in <small>DARPA</small>’s catalogue are capable of spotting <small>AI</small>-generated text, synthesised or edited images, and deepfake audio and video. The results for fake-audio detection are “particularly robust”, Dr Corvey says. And a fake-audio track can, of course, indicate that the accompanying video is also fake. </p><p><small>DARPA</small>’s is not the only effort of this kind. Oren Etzioni, a computer scientist and former head of the Allen Institute for Artificial Intelligence, a research outfit, founded TrueMedia.org, a non-profit group, in January to expand access to detection tools. On April 2nd the group unveiled a website which brings together open-source and commercially available tools, through a free web interface, to provide a one-stop-shop for detection of synthetic or manipulated images, video and audio using multiple tools simultaneously.</p><p>When it comes to bad content, <small>AI</small> is both a sword and shield, notes Nick Clegg, head of global affairs at Meta. For his part, Dr Corvey says he is optimistic that defensive <small>AI</small>-powered detection tools can stay ahead of offensive generation tools. Dr Howard, of Oxford University, agrees—at least for now. This is a “lucky moment” in which technology firms can spot fake videos pretty reliably, he says, “though I don’t know that this is going to last for ever.”</p><p>Renée DiResta, who studies information flows at the Stanford Internet Observatory, is less convinced. Today’s detection tools may work well in a controlled environment when there is plenty of time to make an assessment, she says. But when it comes to making snap judgments in the heat of the moment, “I don’t think the defender is necessarily favoured.” Besides, she observes, there is a much deeper problem. Even if deceptive media can be detected with perfect accuracy, not everyone will believe that a fake video is fake. She cites the example of fake audio clips that went viral just before an election in Slovakia in September 2023, in which a politician was apparently heard discussing election-rigging with a journalist. He later lost the election. “People are highly resistant to fact-checks if they don’t like the fact checker,” she says. That means the mitigation of disinformation will require much more than just technology. <span class="span_ufinish">■</span></p><p class="link_navbar">This article was downloaded by <a href="https://z-lib.io" class="producer_link">zlibrary</a> from <a href="https://www.economist.com/science-and-technology/2024/05/01/disinformation-is-on-the-rise-how-does-it-work" class="origin_link">https://www.economist.com/science-and-technology/2024/05/01/disinformation-is-on-the-rise-how-does-it-work</a> </p>
</body>
</html>
